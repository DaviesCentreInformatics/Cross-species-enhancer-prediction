{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.utils import seq2onehot\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def seq2onehot(in_fa, random_choice=False, rand_n=None):\n",
    "    \"\"\"\n",
    "    Function to generate one-hot encoded matrices from fasta sequences\n",
    "    :param in_fa: Path to multifasta file.\n",
    "    :param random_choice: If true will take a random sample from the fasta record. Default False\n",
    "    :param rand_n: If random_choice = True, will take rand_n number of random samples.\n",
    "    :return: Matrix with  shape N, L, 4.\n",
    "    N = number of records in the fasta file.\n",
    "    L = length of the sequence (rows) and\n",
    "    4 is A, C, G, T.\n",
    "    \"\"\"\n",
    "    print(f'Reading {in_fa} into dictionary  and removing N\\'s')\n",
    "    multi_fa = SeqIO.to_dict(SeqIO.parse(in_fa, 'fasta'))\n",
    "    clean_multi_fa = {}\n",
    "    for k, v in tqdm(multi_fa.items()):\n",
    "        if 'N' not in str(v.seq).upper():\n",
    "            clean_multi_fa[f'{k}'] = v\n",
    "    if random_choice is True:\n",
    "        random.seed(12)\n",
    "        rand_clean_idx = random.sample(list(clean_multi_fa), k=rand_n)\n",
    "        clean_fa = {key: clean_multi_fa[key] for key in rand_clean_idx}\n",
    "        print(f'Number of clean records taken randomly: {rand_n}')\n",
    "    else:\n",
    "        clean_fa = clean_multi_fa\n",
    "        print(f'Number of records before N removal: {len(multi_fa)}\\nNumber of records after N removal {len(clean_fa)}')\n",
    "    seq_len = len(v.seq)\n",
    "    start_time = time.time()\n",
    "    one_hot_mat = np.zeros((len(clean_fa), seq_len, 4))\n",
    "\n",
    "    print(f'Beginning one-hot encoding of {in_fa}')\n",
    "    for k, record in enumerate(tqdm(clean_fa.items())):\n",
    "        for i in range(len(str(record[1].seq).upper())):\n",
    "            assert str(record[1].seq).upper()[i] != 'N', 'Something went ' \\\n",
    "                                                         'wrong prior. You ' \\\n",
    "                                                         'need to make sure ' \\\n",
    "                                                         'there are no Ns in ' \\\n",
    "                                                         'the seq'\n",
    "            if str(record[1].seq).upper()[i] == 'A':\n",
    "                one_hot_mat[k][i][0] = 1.\n",
    "            elif str(record[1].seq).upper()[i] == 'C':\n",
    "                one_hot_mat[k][i][1] = 1.\n",
    "            elif str(record[1].seq).upper()[i] == 'G':\n",
    "                one_hot_mat[k][i][2] = 1.\n",
    "            elif str(record[1].seq).upper()[i] == 'T':\n",
    "                one_hot_mat[k][i][3] = 1.\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f'Time taken to create one-hot matrix: {total_time/60} mins')\n",
    "    return one_hot_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5812it [00:00, 58117.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "534083it [00:08, 63217.67it/s]\n",
      "6071it [00:00, 60703.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152596it [00:02, 69904.37it/s]\n",
      "5992it [00:00, 59877.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76299it [00:01, 59894.22it/s]\n"
     ]
    }
   ],
   "source": [
    "## Deduplicate fasta multifasta file. Seems to be some bug in the shuffle code that means certain records get put in the multifasta file > 1 times. ##\n",
    "split = ['train', 'val', 'test']\n",
    "ids = {}\n",
    "for s in split:\n",
    "    print(f'Deduplicating {s}')\n",
    "    fa = open(f'../data/gene_prediction/ensembl/pos_fa/Bos_taurus.ARS-UCD1.2.105.UTRs.exons.DEDUP.noXY.200bp.{s}.fa', 'w')\n",
    "    for record in tqdm(SeqIO.parse(f'../data/gene_prediction/ensembl/pos_fa/Bos_taurus.ARS-UCD1.2.105.UTRs.exons.noXY.200bp.{s}.fa', 'fasta')):\n",
    "        # Dictionaries are so so so much faster than lists!!!\n",
    "        if record.name in ids.keys():\n",
    "            continue\n",
    "        else:\n",
    "            ids[f'{record.name}'] = None\n",
    "            SeqIO.write(record, fa, 'fasta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on train for Human.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5111044/5111044 [00:05<00:00, 932468.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on train for Human.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1202748/1202748 [00:01<00:00, 767119.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on train for Human.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1234082/1234082 [00:02<00:00, 522550.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on val for Human.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1459567/1459567 [00:01<00:00, 1209528.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on val for Human.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 343389/343389 [00:00<00:00, 775697.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on val for Human.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352017/352017 [00:00<00:00, 550244.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on test for Human.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 729721/729721 [00:00<00:00, 1154409.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on test for Human.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171743/171743 [00:00<00:00, 654164.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on test for Human.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176273/176273 [00:00<00:00, 472093.04it/s]\n"
     ]
    }
   ],
   "source": [
    "windows = [200]\n",
    "split = ['train', 'val', 'test']\n",
    "n_egs = {}\n",
    "for s in split:\n",
    "    for w in windows:\n",
    "        print(f'Working on {s} for Human.')\n",
    "        fa_path = f'../data/gene_prediction/neg_fa/Homo_sapiens.GRCh38.105.non_genes.noXY.GCbalanced.{w}bp.{s}.fa'\n",
    "        neg = SeqIO.to_dict(SeqIO.parse(fa_path,\n",
    "                                    'fasta'))\n",
    "        count = 0\n",
    "        for k, v in tqdm(neg.items()):\n",
    "            if 'N' not in str(v.seq).upper():\n",
    "                count += 1\n",
    "        n_egs[f'{s}_{w}'] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_200': 5111044, 'train_1000': 1202748, 'train_2000': 1234082, 'val_200': 1459567, 'val_1000': 343389, 'val_2000': 352017, 'test_200': 729721, 'test_1000': 171743, 'test_2000': 176273}\n"
     ]
    }
   ],
   "source": [
    "print(n_egs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoding Human test at 200bps.\n",
      "Reading ../data/gene_prediction/ensembl/pos_fa/Bos_taurus.ARS-UCD1.2.105.UTRs.exons.DEDUP.noXY.200bp.test.fa into dictionary  and removing N's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64400/64400 [00:00<00:00, 510750.92it/s]\n",
      "  1%|          | 328/64399 [00:00<00:39, 1638.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records before N removal: 64400\n",
      "Number of records after N removal 64399\n",
      "Beginning one-hot encoding of ../data/gene_prediction/ensembl/pos_fa/Bos_taurus.ARS-UCD1.2.105.UTRs.exons.DEDUP.noXY.200bp.test.fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64399/64399 [00:31<00:00, 2039.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to create one-hot matrix: 0.5261663675308228 mins\n",
      "Saving test at 200bp as one-hot numpy array.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## POSITIVE EXAMPLES ##\n",
    "split = ['test']\n",
    "windows = [200]\n",
    "for s in split:\n",
    "    for w in windows:\n",
    "        #di_key = f'{s}_{w}'\n",
    "        #samples = n_egs[di_key]\n",
    "        #print(f'Samples: {samples}')\n",
    "        print(f'One hot encoding Human {s} at {w}bps.')\n",
    "        #neg = np.load(f'../data/Fantom5/enhancer/pos', mmap_mode='r')\n",
    "        mat = seq2onehot(f'../data/gene_prediction/ensembl/pos_fa/Bos_taurus.ARS-UCD1.2.105.UTRs.exons.DEDUP.noXY.{w}bp.{s}.fa')\n",
    "        print(f'Saving {s} at {w}bp as one-hot numpy array.\\n')\n",
    "        np.save(f'../data/gene_prediction/ensembl/pos_npy/Bos_taurus.ARS-UCD1.2.105.UTRs.exons.DEDUP.noXY.{w}bp.{s}.npy', mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoding test at 200bps.\n",
      "Reading ../data/gene_prediction/ensembl/neg_fa/Bos_taurus.ARS-UCD1.2.105.non_UTRs.exons.DEDUP.noXY.GCbalanced.200bp.test.fa into dictionary  and removing N's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961642/961642 [00:01<00:00, 770311.37it/s]\n",
      "  0%|          | 182/64399 [00:00<00:35, 1814.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clean records taken randomly: 64399\n",
      "Beginning one-hot encoding of ../data/gene_prediction/ensembl/neg_fa/Bos_taurus.ARS-UCD1.2.105.non_UTRs.exons.DEDUP.noXY.GCbalanced.200bp.test.fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64399/64399 [00:31<00:00, 2073.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to create one-hot matrix: 0.5185190518697103 mins\n",
      "Saving test at 200bp as one-hot numpy array.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## NEGATIVE EXAMPLES ##\n",
    "split = ['test']\n",
    "windows = [200]\n",
    "for s in split:\n",
    "    for w in windows:\n",
    "        print(f'One hot encoding {s} at 200bps.')\n",
    "        pos_npy = np.load(f'../data/gene_prediction/ensembl/pos_npy/Bos_taurus.ARS-UCD1.2.105.UTRs.exons.DEDUP.noXY.{w}bp.{s}.npy',\n",
    "                              mmap_mode='r')\n",
    "        mat = seq2onehot(f'../data/gene_prediction/ensembl/neg_fa/Bos_taurus.ARS-UCD1.2.105.non_UTRs.exons.DEDUP.noXY.GCbalanced.{w}bp.{s}.fa',\n",
    "                             random_choice=True,\n",
    "                             rand_n=pos_npy.shape[0])\n",
    "        print(f'Saving {s} at {w}bp as one-hot numpy array.\\n')\n",
    "        np.save(f'../data/gene_prediction/ensembl/neg_npy/Bos_taurus.ARS-UCD1.2.105.non_UTRs.exons.DEDUP.noXY.GCbalanced.{w}bp.{s}.npy', mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Bos_taurus.ARS-UCD1.2.105.\n",
      "Saving Bos_taurus.ARS-UCD1.2.105 test npy files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.01s/it]\n"
     ]
    }
   ],
   "source": [
    "annotation = ['Bos_taurus.ARS-UCD1.2.105']\n",
    "split = ['test']\n",
    "for anno in annotation:\n",
    "    print(f'Working on {anno}.')\n",
    "    for s in tqdm(split):\n",
    "        pos_X = np.load(f'../data/gene_prediction/ensembl/pos_npy/{anno}.UTRs.exons.DEDUP.noXY.200bp.{s}.npy', mmap_mode='r')\n",
    "        neg_X = np.load(f'../data/gene_prediction/ensembl/neg_npy/{anno}.non_UTRs.exons.DEDUP.noXY.GCbalanced.200bp.{s}.npy', mmap_mode='r')\n",
    "\n",
    "        pos_y = np.ones((pos_X.shape[0]))\n",
    "        neg_y = np.zeros((neg_X.shape[0]))\n",
    "\n",
    "        assert pos_X.shape[0] == neg_X.shape[0], 'The dataset is not balanced.'\n",
    "\n",
    "        dat = np.vstack((pos_X, neg_X))\n",
    "        lab = np.hstack((pos_y, neg_y))\n",
    "\n",
    "        print(f'Saving {anno} {s} npy files.')\n",
    "        np.save(f'../data/gene_prediction/ensembl/datasets/{anno}.UTRs.exons.balanced.onehot.200bp.{s}_X.npy', dat)\n",
    "        np.save(f'../data/gene_prediction/ensembl/datasets/{anno}.UTRS.exons.balanced.onehot.200bp.{s}_y.npy', lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking human and mouse datasets together.\n",
      "Saving train array to file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [02:10<04:21, 130.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking human and mouse datasets together.\n",
      "Saving val array to file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [02:52<01:18, 78.63s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking human and mouse datasets together.\n",
      "Saving test array to file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:05<00:00, 61.98s/it]\n"
     ]
    }
   ],
   "source": [
    "split = ['train', 'val', 'test']\n",
    "for s in tqdm(split):\n",
    "    print('Stacking human and mouse datasets together.')\n",
    "    pos_hg38 = np.load(f'../data/gene_prediction/ensembl/pos_npy/Homo_sapiens.GRCh38.105.UTRs.exons.DEDUP.noXY.200bp.{s}.npy', mmap_mode='r')\n",
    "    pos_hg38_y = np.ones(pos_hg38.shape[0])\n",
    "\n",
    "    pos_mm10 = np.load(f'../data/gene_prediction/ensembl/pos_npy/Mus_musculus.GRCm38.102.UTRs.exons.DEDUP.noXY.200bp.{s}.npy', mmap_mode='r')\n",
    "    pos_mm10_y = np.ones(pos_mm10.shape[0])\n",
    "\n",
    "    neg_hg38 = np.load(f'../data/gene_prediction/ensembl/neg_npy/Homo_sapiens.GRCh38.105.non_UTRs.exons.DEDUP.noXY.GCbalanced.200bp.{s}.npy', mmap_mode='r')\n",
    "    neg_hg38_y = np.zeros(neg_hg38.shape[0])\n",
    "\n",
    "    neg_mm10 = np.load(f'../data/gene_prediction/ensembl/neg_npy/Mus_musculus.GRCm38.102.non_UTRs.exons.DEDUP.noXY.GCbalanced.200bp.{s}.npy', mmap_mode='r')\n",
    "    neg_mm10_y = np.zeros(neg_mm10.shape[0])\n",
    "\n",
    "    assert pos_hg38.shape[0] == neg_hg38.shape[0]\n",
    "    assert pos_mm10.shape[0] == neg_mm10.shape[0]\n",
    "\n",
    "    pos_X = np.vstack((pos_hg38, pos_mm10))\n",
    "    pos_y = np.hstack((pos_hg38_y, pos_mm10_y))\n",
    "\n",
    "    neg_X = np.vstack((neg_hg38, neg_mm10))\n",
    "    neg_y = np.hstack((neg_hg38_y, neg_mm10_y))\n",
    "\n",
    "    dat_X = np.vstack((pos_X, neg_X))\n",
    "    dat_y = np.hstack((pos_y, neg_y))\n",
    "\n",
    "    print(f'Saving {s} array to file.')\n",
    "    np.save(f'../data/gene_prediction/ensembl/datasets/Homo.Mus.UTRs.exons.balanced.onehot.200bp.{s}_X.npy', dat_X)\n",
    "    np.save(f'../data/gene_prediction/ensembl/datasets/Homo.Mus.UTRs.exons.balanced.onehot.200bp.{s}_y.npy', dat_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### MOUSE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset for size 200bps using hg19 data.\n",
      "Generating labels for dataset at 200bps\n",
      "Stacking examples and labels together\n",
      "Saving dataset at 200bps.\n",
      "Creating val dataset for size 200bps using hg19 data.\n",
      "Generating labels for dataset at 200bps\n",
      "Stacking examples and labels together\n",
      "Saving dataset at 200bps.\n",
      "Creating test dataset for size 200bps using hg19 data.\n",
      "Generating labels for dataset at 200bps\n",
      "Stacking examples and labels together\n",
      "Saving dataset at 200bps.\n"
     ]
    }
   ],
   "source": [
    "## Create DATASETS ##\n",
    "import numpy as np\n",
    "split = ['train', 'val', 'test']\n",
    "windows = [200]#, 1000]\n",
    "species = ['hg19']\n",
    "for sp in species:\n",
    "    for s in split:\n",
    "        for w in windows:\n",
    "            print(f'Creating {s} dataset for size {w}bps using {sp} data.')\n",
    "            pos_X = np.load(f'../data/SilencerDB/{sp}/pos_npy/{sp}.SilencerDB'\n",
    "                            f'.silencers.noXY.{w}bp.{s}.npy',\n",
    "                        mmap_mode='r')\n",
    "            neg_X = np.load(f'../data/SilencerDB/{sp}/neg_npy/{sp}.SilencerDB'\n",
    "                            f'.non_silencers.noXY.GCbalanced.{w}bp.{s}.npy',\n",
    "                        mmap_mode='r')\n",
    "            print(f'Generating labels for dataset at {w}bps')\n",
    "            pos_y = np.ones(pos_X.shape[0])\n",
    "            neg_y = np.zeros(neg_X.shape[0])\n",
    "\n",
    "            print(f'Stacking examples and labels together')\n",
    "            dat = np.vstack((pos_X, neg_X))\n",
    "            lab = np.hstack((pos_y, neg_y))\n",
    "\n",
    "            print(f'Saving dataset at {w}bps.')\n",
    "            np.save(f'../data/SilencerDB/{sp}/datasets/{sp}.balanced'\n",
    "                    f'.SilencerDB.onehot.{w}bp.{s}_X.npy',\n",
    "                dat)\n",
    "            np.save(f'../data/SilencerDB/{sp}/datasets/{sp}.balanced'\n",
    "                    f'.SilencerDB.onehot.{w}bp.{s}_y.npy', lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining human and mouse train datasets at 200bp.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [10:54<00:00, 654.04s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining human and mouse val datasets at 200bp.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:47<00:00, 167.13s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining human and mouse test datasets at 200bp.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:31<00:00, 91.53s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "split = ['train', 'val', 'test']\n",
    "window = [200]\n",
    "for s in split:\n",
    "    for w in tqdm(window):\n",
    "        print(f'Combining human and mouse {s} datasets at {w}bp.')\n",
    "        human_X = np.load(f'../data/SilencerDB/hg19/datasets/hg19.balanced'\n",
    "                          f'.SilencerDB.onehot.{w}bp.{s}_X.npy',\n",
    "                          mmap_mode='r')\n",
    "        mouse_X = np.load(f'../data/SilencerDB/mm9/datasets/mm9'\n",
    "                          f'.SilencerDB'\n",
    "                          f'.onehot.{w}bp.{s}_X.npy', mmap_mode='r')\n",
    "        human_y = np.load(f'../data/SilencerDB/hg19/datasets/hg19'\n",
    "                          f'.balanced.SilencerDB'\n",
    "                          f'.onehot.{w}bp.{s}_y.npy', mmap_mode='r')\n",
    "        mouse_y = np.load(f'../data/SilencerDB/mm9/datasets/mm9'\n",
    "                          f'.SilencerDB'\n",
    "                          f'.onehot.{w}bp.{s}_y.npy', mmap_mode='r')\n",
    "\n",
    "        comb_X = np.vstack((human_X, mouse_X))\n",
    "        comb_y = np.hstack((human_y, mouse_y))\n",
    "\n",
    "        np.save(f'../data/SilencerDB/hg19.mm9.balanced.SilencerDB.onehot'\n",
    "                f'.{w}bp.{s}_X'\n",
    "                f'.npy', comb_X)\n",
    "        np.save(f'../data/SilencerDB/hg19.mm9.balanced.SilencerDB.onehot'\n",
    "                f'.{w}bp.{s}_y'\n",
    "                f'.npy', comb_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}